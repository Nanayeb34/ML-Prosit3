{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning: Model Training & Evaluation\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1t4z_78XLGZqADzb4eOMoQ3JlFC3c7Ggu?usp=sharing)\n",
    "\n",
    "## Objective\n",
    "Train classical machine learning models for student success prediction:\n",
    "\n",
    "### Classification Models (Logistic Regression & Naive Bayes):\n",
    "1. **Early Warning System - 1st Semester** (4-class)\n",
    "2. **Early Warning System - 1st Year** (4-class)\n",
    "3. **Semester Probation Prediction** (binary)\n",
    "4. **Academic Recovery** (binary)\n",
    "5. **STEM Course Success** (binary)\n",
    "\n",
    "### Regression Model (Linear Regression):\n",
    "6. **Next Semester GPA Change** (continuous)\n",
    "\n",
    "## Evaluation Strategy\n",
    "- **Split**: 80% train, 20% test (stratified for classification)\n",
    "- **Validation**: 5-fold cross-validation on training set\n",
    "- **Class Imbalance**: `class_weight='balanced'` for classification\n",
    "- **Metrics**:\n",
    "  - Classification: F1 (weighted), Balanced Accuracy\n",
    "  - Regression: RMSE, MAE\n",
    "- **Visualization**: Confusion matrices, prediction plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import (\n",
    "    f1_score, balanced_accuracy_score, confusion_matrix,\n",
    "    mean_squared_error, mean_absolute_error, r2_score\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories for outputs\n",
    "Path('models').mkdir(exist_ok=True)\n",
    "Path('results').mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Output directories created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "with open('supervised_learning_datasets.pkl', 'rb') as f:\n",
    "    datasets = pickle.load(f)\n",
    "\n",
    "print(\"Datasets loaded successfully!\")\n",
    "print(f\"\\nNumber of datasets: {len(datasets)}\")\n",
    "print(f\"\\nDataset names:\")\n",
    "for name in datasets.keys():\n",
    "    print(f\"  - {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display dataset summary\n",
    "print(\"=\"*70)\n",
    "print(\"DATASET SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for name, data in datasets.items():\n",
    "    print(f\"\\n{name.replace('_', ' ').title()}:\")\n",
    "    print(f\"  Samples: {len(data['X']):,}\")\n",
    "    print(f\"  Features: {len(data['X'].columns)}\")\n",
    "    \n",
    "    if 'y_multiclass' in data:\n",
    "        print(f\"  Type: Multi-class Classification\")\n",
    "        print(f\"  Classes: {sorted(data['y_multiclass'].unique())}\")\n",
    "        print(f\"  Class distribution:\")\n",
    "        for cls, count in data['y_multiclass'].value_counts().sort_index().items():\n",
    "            print(f\"    {cls}: {count} ({count/len(data['y_multiclass']):.1%})\")\n",
    "    elif data['y'].dtype in ['float64', 'float32']:\n",
    "        print(f\"  Type: Regression\")\n",
    "        print(f\"  Target range: [{data['y'].min():.2f}, {data['y'].max():.2f}]\")\n",
    "        print(f\"  Target mean: {data['y'].mean():.3f} (std: {data['y'].std():.3f})\")\n",
    "    else:\n",
    "        print(f\"  Type: Binary Classification\")\n",
    "        print(f\"  Positive class: {data['y'].sum()} ({data['y'].mean():.1%})\")\n",
    "        print(f\"  Negative class: {(data['y']==0).sum()} ({(data['y']==0).mean():.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classification(y_true, y_pred, model_name, dataset_name, class_labels=None):\n",
    "    \"\"\"\n",
    "    Evaluate classification model and plot confusion matrix.\n",
    "    \n",
    "    Args:\n",
    "        y_true: True labels\n",
    "        y_pred: Predicted labels\n",
    "        model_name: Name of the model (e.g., 'Logistic Regression')\n",
    "        dataset_name: Name of the dataset\n",
    "        class_labels: Optional list of class names for confusion matrix\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary with F1 and Balanced Accuracy scores\n",
    "    \"\"\"\n",
    "    # Calculate metrics\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    bal_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"\\n{model_name} Results:\")\n",
    "    print(f\"  F1 Score (weighted): {f1:.4f}\")\n",
    "    print(f\"  Balanced Accuracy: {bal_acc:.4f}\")\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_labels if class_labels else 'auto',\n",
    "                yticklabels=class_labels if class_labels else 'auto')\n",
    "    plt.title(f'{model_name} - {dataset_name}\\nConfusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    filename = f\"results/{dataset_name.replace(' ', '_')}_{model_name.replace(' ', '_')}_confusion_matrix.png\"\n",
    "    plt.savefig(filename, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return {'f1_weighted': f1, 'balanced_accuracy': bal_acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_regression(y_true, y_pred, model_name, dataset_name):\n",
    "    \"\"\"\n",
    "    Evaluate regression model and plot predictions.\n",
    "    \n",
    "    Args:\n",
    "        y_true: True values\n",
    "        y_pred: Predicted values\n",
    "        model_name: Name of the model\n",
    "        dataset_name: Name of the dataset\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary with RMSE, MAE, and R¬≤ scores\n",
    "    \"\"\"\n",
    "    # Calculate metrics\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"\\n{model_name} Results:\")\n",
    "    print(f\"  RMSE: {rmse:.4f}\")\n",
    "    print(f\"  MAE: {mae:.4f}\")\n",
    "    print(f\"  R¬≤ Score: {r2:.4f}\")\n",
    "    \n",
    "    # Plot predictions vs actual\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Scatter plot\n",
    "    axes[0].scatter(y_true, y_pred, alpha=0.5, s=20)\n",
    "    axes[0].plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], \n",
    "                 'r--', lw=2, label='Perfect Prediction')\n",
    "    axes[0].set_xlabel('True Values')\n",
    "    axes[0].set_ylabel('Predicted Values')\n",
    "    axes[0].set_title(f'{model_name} - {dataset_name}\\nPredictions vs Actual')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    # Residuals\n",
    "    residuals = y_true - y_pred\n",
    "    axes[1].scatter(y_pred, residuals, alpha=0.5, s=20)\n",
    "    axes[1].axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "    axes[1].set_xlabel('Predicted Values')\n",
    "    axes[1].set_ylabel('Residuals')\n",
    "    axes[1].set_title('Residual Plot')\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    filename = f\"results/{dataset_name.replace(' ', '_')}_{model_name.replace(' ', '_')}_predictions.png\"\n",
    "    plt.savefig(filename, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return {'rmse': rmse, 'mae': mae, 'r2': r2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, scaler, metadata, filename):\n",
    "    \"\"\"\n",
    "    Save trained model with scaler and metadata.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model object\n",
    "        scaler: Fitted StandardScaler\n",
    "        metadata: Dictionary with model information\n",
    "        filename: Filename to save (without path)\n",
    "    \"\"\"\n",
    "    model_artifact = {\n",
    "        'model': model,\n",
    "        'scaler': scaler,\n",
    "        'metadata': metadata\n",
    "    }\n",
    "    \n",
    "    filepath = f'models/{filename}'\n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump(model_artifact, f)\n",
    "    \n",
    "    print(f\"\\n  ‚úÖ Model saved: {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize results storage\n",
    "all_results = []\n",
    "\n",
    "print(\"Helper functions defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Early Warning System - 1st Semester\n",
    "\n",
    "Predict final grade bin (Probation/At-Risk/Good Standing/Dean's List) from first semester performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"EARLY WARNING SYSTEM - 1ST SEMESTER\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get data\n",
    "data = datasets['early_warning_1st_semester']\n",
    "X = data['X']\n",
    "y = data['y_multiclass']  # 4-class: Probation, At_Risk, Good_Standing, Deans_List\n",
    "\n",
    "# Get class labels\n",
    "class_labels = sorted(y.unique())\n",
    "print(f\"\\nClasses: {class_labels}\")\n",
    "print(f\"Samples: {len(X):,}\")\n",
    "print(f\"Features: {len(X.columns)}\")\n",
    "\n",
    "# Handle any missing values\n",
    "X_filled = X.fillna(X.median())\n",
    "\n",
    "# Train/test split (stratified)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_filled, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain set: {len(X_train):,} | Test set: {len(X_test):,}\")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression with class_weight='balanced'\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Training Logistic Regression...\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "lr_model = LogisticRegression(\n",
    "    class_weight='balanced',\n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 5-fold CV on training set\n",
    "cv_scores = cross_val_score(\n",
    "    lr_model, X_train_scaled, y_train,\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    scoring='f1_weighted'\n",
    ")\n",
    "\n",
    "print(f\"\\n5-Fold CV F1 Scores: {cv_scores}\")\n",
    "print(f\"Mean CV F1: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n",
    "\n",
    "# Train on full training set\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_lr = lr_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate\n",
    "lr_metrics = evaluate_classification(\n",
    "    y_test, y_pred_lr,\n",
    "    'Logistic Regression',\n",
    "    'Early Warning 1st Semester',\n",
    "    class_labels=class_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Training Naive Bayes...\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "nb_model = GaussianNB()\n",
    "\n",
    "# 5-fold CV on training set\n",
    "cv_scores = cross_val_score(\n",
    "    nb_model, X_train_scaled, y_train,\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    scoring='f1_weighted'\n",
    ")\n",
    "\n",
    "print(f\"\\n5-Fold CV F1 Scores: {cv_scores}\")\n",
    "print(f\"Mean CV F1: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n",
    "\n",
    "# Train on full training set\n",
    "nb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_nb = nb_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate\n",
    "nb_metrics = evaluate_classification(\n",
    "    y_test, y_pred_nb,\n",
    "    'Naive Bayes',\n",
    "    'Early Warning 1st Semester',\n",
    "    class_labels=class_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare and save best model\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nLogistic Regression - F1: {lr_metrics['f1_weighted']:.4f} | Bal Acc: {lr_metrics['balanced_accuracy']:.4f}\")\n",
    "print(f\"Naive Bayes         - F1: {nb_metrics['f1_weighted']:.4f} | Bal Acc: {nb_metrics['balanced_accuracy']:.4f}\")\n",
    "\n",
    "# Save best model\n",
    "if lr_metrics['f1_weighted'] >= nb_metrics['f1_weighted']:\n",
    "    best_model = lr_model\n",
    "    best_name = 'Logistic Regression'\n",
    "    best_metrics = lr_metrics\n",
    "else:\n",
    "    best_model = nb_model\n",
    "    best_name = 'Naive Bayes'\n",
    "    best_metrics = nb_metrics\n",
    "\n",
    "print(f\"\\nüèÜ Best Model: {best_name}\")\n",
    "\n",
    "metadata = {\n",
    "    'dataset': 'early_warning_1st_semester',\n",
    "    'model_type': best_name,\n",
    "    'n_samples': len(X),\n",
    "    'n_features': len(X.columns),\n",
    "    'metrics': best_metrics,\n",
    "    'classes': class_labels\n",
    "}\n",
    "\n",
    "save_model(best_model, scaler, metadata, 'early_warning_1st_semester_best.pkl')\n",
    "\n",
    "# Store results\n",
    "all_results.append({\n",
    "    'Dataset': 'Early Warning 1st Semester',\n",
    "    'Type': 'Multi-class (4)',\n",
    "    'Best Model': best_name,\n",
    "    'F1 (weighted)': best_metrics['f1_weighted'],\n",
    "    'Balanced Accuracy': best_metrics['balanced_accuracy']\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Early Warning System - 1st Year\n",
    "\n",
    "Predict final grade bin from first year (2 semesters) performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"EARLY WARNING SYSTEM - 1ST YEAR\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "data = datasets['early_warning_1st_year']\n",
    "X = data['X']\n",
    "y = data['y_multiclass']\n",
    "\n",
    "class_labels = sorted(y.unique())\n",
    "print(f\"\\nClasses: {class_labels}\")\n",
    "print(f\"Samples: {len(X):,}\")\n",
    "print(f\"Features: {len(X.columns)}\")\n",
    "\n",
    "X_filled = X.fillna(X.median())\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_filled, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain set: {len(X_train):,} | Test set: {len(X_test):,}\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Logistic Regression\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Training Logistic Regression...\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "lr_model = LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)\n",
    "cv_scores = cross_val_score(\n",
    "    lr_model, X_train_scaled, y_train,\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    scoring='f1_weighted'\n",
    ")\n",
    "print(f\"\\nMean CV F1: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n",
    "\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "y_pred_lr = lr_model.predict(X_test_scaled)\n",
    "lr_metrics = evaluate_classification(y_test, y_pred_lr, 'Logistic Regression', 'Early Warning 1st Year', class_labels)\n",
    "\n",
    "# Naive Bayes\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Training Naive Bayes...\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "nb_model = GaussianNB()\n",
    "cv_scores = cross_val_score(\n",
    "    nb_model, X_train_scaled, y_train,\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    scoring='f1_weighted'\n",
    ")\n",
    "print(f\"\\nMean CV F1: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n",
    "\n",
    "nb_model.fit(X_train_scaled, y_train)\n",
    "y_pred_nb = nb_model.predict(X_test_scaled)\n",
    "nb_metrics = evaluate_classification(y_test, y_pred_nb, 'Naive Bayes', 'Early Warning 1st Year', class_labels)\n",
    "\n",
    "# Save best\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nLogistic Regression - F1: {lr_metrics['f1_weighted']:.4f} | Bal Acc: {lr_metrics['balanced_accuracy']:.4f}\")\n",
    "print(f\"Naive Bayes         - F1: {nb_metrics['f1_weighted']:.4f} | Bal Acc: {nb_metrics['balanced_accuracy']:.4f}\")\n",
    "\n",
    "if lr_metrics['f1_weighted'] >= nb_metrics['f1_weighted']:\n",
    "    best_model, best_name, best_metrics = lr_model, 'Logistic Regression', lr_metrics\n",
    "else:\n",
    "    best_model, best_name, best_metrics = nb_model, 'Naive Bayes', nb_metrics\n",
    "\n",
    "print(f\"\\nüèÜ Best Model: {best_name}\")\n",
    "\n",
    "metadata = {\n",
    "    'dataset': 'early_warning_1st_year',\n",
    "    'model_type': best_name,\n",
    "    'n_samples': len(X),\n",
    "    'n_features': len(X.columns),\n",
    "    'metrics': best_metrics,\n",
    "    'classes': class_labels\n",
    "}\n",
    "save_model(best_model, scaler, metadata, 'early_warning_1st_year_best.pkl')\n",
    "\n",
    "all_results.append({\n",
    "    'Dataset': 'Early Warning 1st Year',\n",
    "    'Type': 'Multi-class (4)',\n",
    "    'Best Model': best_name,\n",
    "    'F1 (weighted)': best_metrics['f1_weighted'],\n",
    "    'Balanced Accuracy': best_metrics['balanced_accuracy']\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Semester-by-Semester Probation Prediction\n",
    "\n",
    "Predict if a student will be on probation (CGPA < 2.0) at the end of current semester."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"SEMESTER PROBATION PREDICTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "data = datasets['semester_probation_prediction']\n",
    "X = data['X']\n",
    "y = data['y']  # Binary: 0 = Not on probation, 1 = On probation\n",
    "\n",
    "print(f\"\\nSamples: {len(X):,}\")\n",
    "print(f\"Features: {len(X.columns)}\")\n",
    "print(f\"Class distribution:\")\n",
    "print(f\"  Not on Probation: {(y==0).sum()} ({(y==0).mean():.1%})\")\n",
    "print(f\"  On Probation: {y.sum()} ({y.mean():.1%})\")\n",
    "\n",
    "X_filled = X.fillna(X.median())\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_filled, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain set: {len(X_train):,} | Test set: {len(X_test):,}\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Logistic Regression\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Training Logistic Regression...\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "lr_model = LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)\n",
    "cv_scores = cross_val_score(\n",
    "    lr_model, X_train_scaled, y_train,\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    scoring='f1_weighted'\n",
    ")\n",
    "print(f\"\\nMean CV F1: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n",
    "\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "y_pred_lr = lr_model.predict(X_test_scaled)\n",
    "lr_metrics = evaluate_classification(y_test, y_pred_lr, 'Logistic Regression', 'Probation Prediction', ['Not on Probation', 'On Probation'])\n",
    "\n",
    "# Naive Bayes\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Training Naive Bayes...\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "nb_model = GaussianNB()\n",
    "cv_scores = cross_val_score(\n",
    "    nb_model, X_train_scaled, y_train,\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    scoring='f1_weighted'\n",
    ")\n",
    "print(f\"\\nMean CV F1: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n",
    "\n",
    "nb_model.fit(X_train_scaled, y_train)\n",
    "y_pred_nb = nb_model.predict(X_test_scaled)\n",
    "nb_metrics = evaluate_classification(y_test, y_pred_nb, 'Naive Bayes', 'Probation Prediction', ['Not on Probation', 'On Probation'])\n",
    "\n",
    "# Save best\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nLogistic Regression - F1: {lr_metrics['f1_weighted']:.4f} | Bal Acc: {lr_metrics['balanced_accuracy']:.4f}\")\n",
    "print(f\"Naive Bayes         - F1: {nb_metrics['f1_weighted']:.4f} | Bal Acc: {nb_metrics['balanced_accuracy']:.4f}\")\n",
    "\n",
    "if lr_metrics['f1_weighted'] >= nb_metrics['f1_weighted']:\n",
    "    best_model, best_name, best_metrics = lr_model, 'Logistic Regression', lr_metrics\n",
    "else:\n",
    "    best_model, best_name, best_metrics = nb_model, 'Naive Bayes', nb_metrics\n",
    "\n",
    "print(f\"\\nüèÜ Best Model: {best_name}\")\n",
    "\n",
    "metadata = {\n",
    "    'dataset': 'semester_probation_prediction',\n",
    "    'model_type': best_name,\n",
    "    'n_samples': len(X),\n",
    "    'n_features': len(X.columns),\n",
    "    'metrics': best_metrics\n",
    "}\n",
    "save_model(best_model, scaler, metadata, 'probation_prediction_best.pkl')\n",
    "\n",
    "all_results.append({\n",
    "    'Dataset': 'Probation Prediction',\n",
    "    'Type': 'Binary',\n",
    "    'Best Model': best_name,\n",
    "    'F1 (weighted)': best_metrics['f1_weighted'],\n",
    "    'Balanced Accuracy': best_metrics['balanced_accuracy']\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Academic Recovery Prediction\n",
    "\n",
    "For students on probation (CGPA < 2.0), predict if they will recover (CGPA ‚â• 2.0) in next semester."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ACADEMIC RECOVERY PREDICTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "data = datasets['academic_recovery']\n",
    "X = data['X']\n",
    "y = data['y']  # Binary: 0 = Did not recover, 1 = Recovered\n",
    "\n",
    "print(f\"\\nSamples: {len(X):,} (probation instances)\")\n",
    "print(f\"Features: {len(X.columns)}\")\n",
    "print(f\"Class distribution:\")\n",
    "print(f\"  Did not Recover: {(y==0).sum()} ({(y==0).mean():.1%})\")\n",
    "print(f\"  Recovered: {y.sum()} ({y.mean():.1%})\")\n",
    "\n",
    "X_filled = X.fillna(X.median())\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_filled, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain set: {len(X_train):,} | Test set: {len(X_test):,}\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Logistic Regression\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Training Logistic Regression...\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "lr_model = LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)\n",
    "cv_scores = cross_val_score(\n",
    "    lr_model, X_train_scaled, y_train,\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    scoring='f1_weighted'\n",
    ")\n",
    "print(f\"\\nMean CV F1: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n",
    "\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "y_pred_lr = lr_model.predict(X_test_scaled)\n",
    "lr_metrics = evaluate_classification(y_test, y_pred_lr, 'Logistic Regression', 'Academic Recovery', ['Did not Recover', 'Recovered'])\n",
    "\n",
    "# Naive Bayes\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Training Naive Bayes...\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "nb_model = GaussianNB()\n",
    "cv_scores = cross_val_score(\n",
    "    nb_model, X_train_scaled, y_train,\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    scoring='f1_weighted'\n",
    ")\n",
    "print(f\"\\nMean CV F1: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n",
    "\n",
    "nb_model.fit(X_train_scaled, y_train)\n",
    "y_pred_nb = nb_model.predict(X_test_scaled)\n",
    "nb_metrics = evaluate_classification(y_test, y_pred_nb, 'Naive Bayes', 'Academic Recovery', ['Did not Recover', 'Recovered'])\n",
    "\n",
    "# Save best\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nLogistic Regression - F1: {lr_metrics['f1_weighted']:.4f} | Bal Acc: {lr_metrics['balanced_accuracy']:.4f}\")\n",
    "print(f\"Naive Bayes         - F1: {nb_metrics['f1_weighted']:.4f} | Bal Acc: {nb_metrics['balanced_accuracy']:.4f}\")\n",
    "\n",
    "if lr_metrics['f1_weighted'] >= nb_metrics['f1_weighted']:\n",
    "    best_model, best_name, best_metrics = lr_model, 'Logistic Regression', lr_metrics\n",
    "else:\n",
    "    best_model, best_name, best_metrics = nb_model, 'Naive Bayes', nb_metrics\n",
    "\n",
    "print(f\"\\nüèÜ Best Model: {best_name}\")\n",
    "\n",
    "metadata = {\n",
    "    'dataset': 'academic_recovery',\n",
    "    'model_type': best_name,\n",
    "    'n_samples': len(X),\n",
    "    'n_features': len(X.columns),\n",
    "    'metrics': best_metrics\n",
    "}\n",
    "save_model(best_model, scaler, metadata, 'academic_recovery_best.pkl')\n",
    "\n",
    "all_results.append({\n",
    "    'Dataset': 'Academic Recovery',\n",
    "    'Type': 'Binary',\n",
    "    'Best Model': best_name,\n",
    "    'F1 (weighted)': best_metrics['f1_weighted'],\n",
    "    'Balanced Accuracy': best_metrics['balanced_accuracy']\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Next Semester GPA Change (Regression)\n",
    "\n",
    "Predict the change in GPA in the next semester (continuous value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"NEXT SEMESTER GPA CHANGE - REGRESSION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "data = datasets['next_semester_gpa_change']\n",
    "X = data['X']\n",
    "y = data['y']  # Continuous: GPA change\n",
    "\n",
    "print(f\"\\nSamples: {len(X):,}\")\n",
    "print(f\"Features: {len(X.columns)}\")\n",
    "print(f\"Target statistics:\")\n",
    "print(f\"  Range: [{y.min():.2f}, {y.max():.2f}]\")\n",
    "print(f\"  Mean: {y.mean():.3f} (std: {y.std():.3f})\")\n",
    "\n",
    "X_filled = X.fillna(X.median())\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_filled, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain set: {len(X_train):,} | Test set: {len(X_test):,}\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Linear Regression\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Training Linear Regression...\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "lr_model = LinearRegression()\n",
    "\n",
    "# 5-fold CV on training set\n",
    "cv_scores = cross_val_score(\n",
    "    lr_model, X_train_scaled, y_train,\n",
    "    cv=KFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    scoring='neg_mean_squared_error'\n",
    ")\n",
    "cv_rmse = np.sqrt(-cv_scores)\n",
    "print(f\"\\nMean CV RMSE: {cv_rmse.mean():.4f} (+/- {cv_rmse.std():.4f})\")\n",
    "\n",
    "# Train on full training set\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = lr_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate\n",
    "metrics = evaluate_regression(y_test, y_pred, 'Linear Regression', 'Next Semester GPA Change')\n",
    "\n",
    "# Save model\n",
    "metadata = {\n",
    "    'dataset': 'next_semester_gpa_change',\n",
    "    'model_type': 'Linear Regression',\n",
    "    'n_samples': len(X),\n",
    "    'n_features': len(X.columns),\n",
    "    'metrics': metrics\n",
    "}\n",
    "save_model(lr_model, scaler, metadata, 'next_semester_gpa_change.pkl')\n",
    "\n",
    "all_results.append({\n",
    "    'Dataset': 'Next Semester GPA Change',\n",
    "    'Type': 'Regression',\n",
    "    'Best Model': 'Linear Regression',\n",
    "    'RMSE': metrics['rmse'],\n",
    "    'MAE': metrics['mae'],\n",
    "    'R¬≤': metrics['r2']\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 STEM Course Success Prediction\n",
    "\n",
    "Predict if a student will pass a STEM course (Grade ‚â• 2.0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"STEM COURSE SUCCESS PREDICTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "data = datasets['stem_course_success']\n",
    "X = data['X']\n",
    "y = data['y']  # Binary: 0 = Failed, 1 = Passed\n",
    "\n",
    "print(f\"\\nSamples: {len(X):,} (course enrollments)\")\n",
    "print(f\"Features: {len(X.columns)}\")\n",
    "print(f\"Class distribution:\")\n",
    "print(f\"  Failed: {(y==0).sum()} ({(y==0).mean():.1%})\")\n",
    "print(f\"  Passed: {y.sum()} ({y.mean():.1%})\")\n",
    "\n",
    "X_filled = X.fillna(X.median())\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_filled, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain set: {len(X_train):,} | Test set: {len(X_test):,}\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Logistic Regression\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Training Logistic Regression...\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "lr_model = LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)\n",
    "cv_scores = cross_val_score(\n",
    "    lr_model, X_train_scaled, y_train,\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    scoring='f1_weighted'\n",
    ")\n",
    "print(f\"\\nMean CV F1: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n",
    "\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "y_pred_lr = lr_model.predict(X_test_scaled)\n",
    "lr_metrics = evaluate_classification(y_test, y_pred_lr, 'Logistic Regression', 'STEM Course Success', ['Failed', 'Passed'])\n",
    "\n",
    "# Naive Bayes\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Training Naive Bayes...\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "nb_model = GaussianNB()\n",
    "cv_scores = cross_val_score(\n",
    "    nb_model, X_train_scaled, y_train,\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    scoring='f1_weighted'\n",
    ")\n",
    "print(f\"\\nMean CV F1: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n",
    "\n",
    "nb_model.fit(X_train_scaled, y_train)\n",
    "y_pred_nb = nb_model.predict(X_test_scaled)\n",
    "nb_metrics = evaluate_classification(y_test, y_pred_nb, 'Naive Bayes', 'STEM Course Success', ['Failed', 'Passed'])\n",
    "\n",
    "# Save best\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nLogistic Regression - F1: {lr_metrics['f1_weighted']:.4f} | Bal Acc: {lr_metrics['balanced_accuracy']:.4f}\")\n",
    "print(f\"Naive Bayes         - F1: {nb_metrics['f1_weighted']:.4f} | Bal Acc: {nb_metrics['balanced_accuracy']:.4f}\")\n",
    "\n",
    "if lr_metrics['f1_weighted'] >= nb_metrics['f1_weighted']:\n",
    "    best_model, best_name, best_metrics = lr_model, 'Logistic Regression', lr_metrics\n",
    "else:\n",
    "    best_model, best_name, best_metrics = nb_model, 'Naive Bayes', nb_metrics\n",
    "\n",
    "print(f\"\\nüèÜ Best Model: {best_name}\")\n",
    "\n",
    "metadata = {\n",
    "    'dataset': 'stem_course_success',\n",
    "    'model_type': best_name,\n",
    "    'n_samples': len(X),\n",
    "    'n_features': len(X.columns),\n",
    "    'metrics': best_metrics\n",
    "}\n",
    "save_model(best_model, scaler, metadata, 'stem_course_success_best.pkl')\n",
    "\n",
    "all_results.append({\n",
    "    'Dataset': 'STEM Course Success',\n",
    "    'Type': 'Binary',\n",
    "    'Best Model': best_name,\n",
    "    'F1 (weighted)': best_metrics['f1_weighted'],\n",
    "    'Balanced Accuracy': best_metrics['balanced_accuracy']\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Summary & Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"FINAL MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Save to CSV\n",
    "results_df.to_csv('model_summary.csv', index=False)\n",
    "print(\"\\n‚úÖ Summary saved to 'model_summary.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# Classification models\n",
    "classification_df = results_df[results_df['Type'].str.contains('class')].copy()\n",
    "x_pos = np.arange(len(classification_df))\n",
    "\n",
    "axes[0].bar(x_pos - 0.2, classification_df['F1 (weighted)'], 0.4, label='F1 (weighted)', alpha=0.8)\n",
    "axes[0].bar(x_pos + 0.2, classification_df['Balanced Accuracy'], 0.4, label='Balanced Accuracy', alpha=0.8)\n",
    "axes[0].set_xlabel('Dataset')\n",
    "axes[0].set_ylabel('Score')\n",
    "axes[0].set_title('Classification Models Performance')\n",
    "axes[0].set_xticks(x_pos)\n",
    "axes[0].set_xticklabels(classification_df['Dataset'], rotation=45, ha='right')\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "axes[0].set_ylim([0, 1.1])\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (f1, ba) in enumerate(zip(classification_df['F1 (weighted)'], classification_df['Balanced Accuracy'])):\n",
    "    axes[0].text(i - 0.2, f1 + 0.02, f'{f1:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "    axes[0].text(i + 0.2, ba + 0.02, f'{ba:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# Regression model\n",
    "regression_df = results_df[results_df['Type'] == 'Regression'].copy()\n",
    "if len(regression_df) > 0:\n",
    "    x_pos_reg = np.arange(len(regression_df))\n",
    "    \n",
    "    axes[1].bar(x_pos_reg - 0.3, regression_df['RMSE'], 0.3, label='RMSE', alpha=0.8)\n",
    "    axes[1].bar(x_pos_reg, regression_df['MAE'], 0.3, label='MAE', alpha=0.8)\n",
    "    axes[1].bar(x_pos_reg + 0.3, regression_df['R¬≤'], 0.3, label='R¬≤', alpha=0.8)\n",
    "    axes[1].set_xlabel('Dataset')\n",
    "    axes[1].set_ylabel('Score')\n",
    "    axes[1].set_title('Regression Model Performance')\n",
    "    axes[1].set_xticks(x_pos_reg)\n",
    "    axes[1].set_xticklabels(regression_df['Dataset'], rotation=45, ha='right')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (rmse, mae, r2) in enumerate(zip(regression_df['RMSE'], regression_df['MAE'], regression_df['R¬≤'])):\n",
    "        axes[1].text(i - 0.3, rmse + 0.02, f'{rmse:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "        axes[1].text(i, mae + 0.02, f'{mae:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "        axes[1].text(i + 0.3, r2 + 0.02, f'{r2:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/model_comparison_summary.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Comparison visualization saved to 'results/model_comparison_summary.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Best Practices Applied:\n",
    "1. ‚úÖ Stratified splits for classification (maintains class proportions)\n",
    "2. ‚úÖ Class weighting to handle imbalance\n",
    "3. ‚úÖ 5-fold cross-validation for robust evaluation\n",
    "4. ‚úÖ Feature scaling (StandardScaler)\n",
    "5. ‚úÖ Comprehensive metrics (F1 weighted, Balanced Accuracy, RMSE, MAE)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
